# scripts/run_emotion_kluebert_v2_postprocess.py
# âœ… ì‹¤í–‰: python scripts/run_emotion_kluebert_v2_postprocess.py
#
# ì…ë ¥:
# - INPUT_PATHê°€ íŒŒì¼(.jsonl)ì´ë©´: ê·¸ íŒŒì¼ 1ê°œ ì²˜ë¦¬
# - INPUT_PATHê°€ í´ë”ë©´: (ìŠ¤ìœ„ì¹˜ì— ë”°ë¼) victim_offender_pair_*.jsonl / victim_thoughts_pair_*.jsonl ì²˜ë¦¬
#
# ì¶œë ¥(1:1):
# - out íŒŒì¼ëª…ì€ "pred_" + ì›ë³¸ íŒŒì¼ëª…
#   ì˜ˆ) victim_offender_pair_<caseId>.jsonl -> pred_victim_offender_pair_<caseId>.jsonl
#   ì˜ˆ) victim_thoughts_pair_<caseId>.jsonl -> pred_victim_thoughts_pair_<caseId>.jsonl
#
# í•µì‹¬:
# - KLUE-BERT v2 ëª¨ë¸(7ê°ì •)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ probs7/pred7ì„ ì–»ê³ 
# - í›„ì²˜ë¦¬ ê·œì¹™ìœ¼ë¡œ 4ê°ì •(N/F/A/E)ë¡œ ë³€í™˜
# - íŠ¹íˆ "ë†€ëŒ"ì€ ìœ„í˜‘/ë³´ìƒ ë‹¨ì„œë¡œ F/E/N ì¤‘ í•˜ë‚˜ë¡œ ë¶„ê¸°

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Windowsì—ì„œ ê²½ê³  ì¤„ì´ê¸°(ì„ íƒ)
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"

# =========================
# ğŸ”§ ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë¨
# =========================
INPUT_PATH = r"C:\LIT_VP2\VP\scripts\datasets"          # íŒŒì¼(.jsonl) ë˜ëŠ” í´ë”
OUTPUT_DIR = r"C:\LIT_VP2\VP\scripts\emotion_result_klubert"    # ì¶œë ¥ í´ë”

# âœ… ìŠ¤ìœ„ì¹˜: ì›í•˜ëŠ” ê²ƒë§Œ Trueë¡œ ì¼œê¸°
RUN_OFFENDER_PAIR = False     # victim_offender_pair_*.jsonl ì²˜ë¦¬
RUN_THOUGHTS_PAIR = True    # victim_thoughts_pair_*.jsonl ì²˜ë¦¬

BATCH_SIZE = 16
MAX_LENGTH = 256

# âœ… KLUE BERT v2 ê°ì •ëª¨ë¸(7-class)
MODEL_ID = "dlckdfuf141/korean-emotion-kluebert-v2"
TOKENIZER_ID = "dlckdfuf141/korean-emotion-kluebert-v2"

# =========================
# 7-class ë¼ë²¨ ì •ì˜
# (ëª¨ë¸ ì¹´ë“œ ê¸°ì¤€: 0ê³µí¬,1ë†€ëŒ,2ë¶„ë…¸,3ìŠ¬í””,4ì¤‘ë¦½,5í–‰ë³µ,6í˜ì˜¤)
# =========================
ID2LABEL_7 = {
    0: "FEAR",      # ê³µí¬
    1: "SURPRISE",  # ë†€ëŒ
    2: "ANGER",     # ë¶„ë…¸
    3: "SAD",       # ìŠ¬í””
    4: "NEUTRAL",   # ì¤‘ë¦½
    5: "HAPPY",     # í–‰ë³µ
    6: "DISGUST",   # í˜ì˜¤
}

# =========================
# 7 -> 4 ê¸°ë³¸ ë§¤í•‘(ë†€ëŒì€ ê·œì¹™ìœ¼ë¡œ ì²˜ë¦¬)
# =========================
# - ë…¼ë¬¸ Emoti-Shing ê´€ì ì—ì„œ:
#   N(Neutral), F(Fear), A(Anger), E(Excitement)
# - ê¸°ë³¸ì ìœ¼ë¡œ:
#   FEAR->F, ANGER->A, NEUTRAL->N, HAPPY->E
#   DISGUSTëŠ” ê°•í•œ ê±°ë¶€/ë°˜ê°ì´ë¼ Aë¡œ í•©ì¹˜ëŠ” í¸ì´ ì‹¤ì „ì—ì„œ ì•ˆì •ì 
#   SADëŠ” ì €ê°ì„± ë¶€ì •ì´ë¼ Nìœ¼ë¡œ ë‘ëŠ” ê¸°ë³¸(ì›í•˜ë©´ Fë¡œ ë°”ê¿”ë„ ë¨)
MAP_7_TO_4_BASE = {
    "FEAR": "F",
    "ANGER": "A",
    "NEUTRAL": "N",
    "HAPPY": "E",
    "DISGUST": "A",
    "SAD": "N",         # í•„ìš”í•˜ë©´ "F"ë¡œ ë³€ê²½ ê°€ëŠ¥
    "SURPRISE": None,   # í›„ì²˜ë¦¬ ê·œì¹™ì—ì„œ ê²°ì •
}

# =========================
# "ë†€ëŒ" ë¶„ê¸°ìš© í‚¤ì›Œë“œ(í›„ì²˜ë¦¬ ê·œì¹™)
# - ìœ„í˜‘ ë‹¨ì„œ: ê³µí¬(F)ë¡œ ë³´ëƒ„
# - ë³´ìƒ ë‹¨ì„œ: í¥ë¶„/ê¸°ëŒ€(E)ë¡œ ë³´ëƒ„
# - ë‘˜ ë‹¤ ì—†ìœ¼ë©´: Nìœ¼ë¡œ ë³´ëƒ„
# =========================
THREAT_CUES = [
    "ê²€ì°°", "ê²€ì‚¬", "ìˆ˜ì‚¬", "ê²½ì°°", "ê¸ˆê°ì›", "ê¸ˆìœµê°ë…ì›", "ì§€ê²€", "ì§€ì²­",
    "ì—°ë£¨", "ë²”ì£„", "í˜ì˜", "í”¼ì˜ì", "ê³ ì†Œ", "ê³ ë°œ", "ì˜ì¥", "ì²´í¬", "êµ¬ì†",
    "ì••ìˆ˜", "ëª°ìˆ˜", "ì†¡ì¹˜", "ê¸°ì†Œ", "ì¬íŒ", "ë²Œê¸ˆ", "ì²˜ë²Œ",
    "ë™ê²°", "ì •ì§€", "ì°¨ë‹¨", "ê±°ë˜ì •ì§€", "ê³„ì¢Œì •ì§€", "ëŒ€í¬í†µì¥",
    "ìœ„í—˜", "ê¸´ê¸‰", "ì¦‰ì‹œ", "ì˜¤ëŠ˜ ì•ˆì—", "ì§€ê¸ˆ ë‹¹ì¥", "í°ì¼", "ë¬¸ì œ",
]

REWARD_CUES = [
    "í™˜ê¸‰", "ë‹¹ì²¨", "ì´ë“", "í˜œíƒ", "ì§€ì›ê¸ˆ", "ë³´ìƒ", "ë¦¬ì›Œë“œ", "ìºì‹œë°±",
    "ìŠ¹ì¸", "ëŒ€ì¶œìŠ¹ì¸", "í•œë„", "ê¸ˆë¦¬", "ìš°ëŒ€", "ìˆ˜ìˆ˜ë£Œ ë©´ì œ",
    "ì…ê¸ˆ", "ì§€ê¸‰", "ë°›ìœ¼ì‹¤", "ë‚˜ì˜µë‹ˆë‹¤", "í•´ê²°", "ì•ˆì‹¬", "ê´œì°®ìŠµë‹ˆë‹¤",
    "ì¢‹ì€ ì†Œì‹", "ê¸°íšŒ", "ê°€ëŠ¥í•©ë‹ˆë‹¤",
]

# (ì„ íƒ) ë†€ëŒì„ â€œë¶„ê¸°â€í• ì§€ ì—¬ë¶€ë¥¼ ì¢€ ë” ì—„ê²©íˆ í•˜ê³  ì‹¶ìœ¼ë©´ threshold ì‚¬ìš© ê°€ëŠ¥
# ì˜ˆ: ë†€ëŒ í™•ë¥ ì´ 0.40 ì´ìƒì¼ ë•Œë§Œ ë¶„ê¸°í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ë§¤í•‘(ë˜ëŠ” N)ìœ¼ë¡œ ì²˜ë¦¬
SURPRISE_MIN_PROB = 0.0  # 0.0ì´ë©´ ë†€ëŒì´ topì´ë“  ì•„ë‹ˆë“  ê·œì¹™ì„ ì ìš©í•  ìˆ˜ ìˆìŒ(ì•„ë˜ ë¡œì§ ì°¸ê³ )


def load_lines(path: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    for line in path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        rows.append(json.loads(line))
    return rows


def write_jsonl(rows: List[Dict[str, Any]], path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r, ensure_ascii=False) + "\n")


def get_target_globs() -> List[str]:
    """ìŠ¤ìœ„ì¹˜ì— ë”°ë¼ í´ë” ì…ë ¥ ì‹œ ì²˜ë¦¬í•  íŒŒì¼ íŒ¨í„´ ê²°ì •"""
    globs: List[str] = []
    if RUN_OFFENDER_PAIR:
        globs.append("victim_offender_pair_*.jsonl")
    if RUN_THOUGHTS_PAIR:
        globs.append("victim_thoughts_pair_*.jsonl")
    return globs


def iter_input_files(input_path: Path) -> Iterable[Path]:
    """
    - íŒŒì¼ì´ë©´ ê·¸ íŒŒì¼ 1ê°œ
    - í´ë”ë©´ ìŠ¤ìœ„ì¹˜ì— ë”°ë¼ íƒ€ê²Ÿ íŒ¨í„´ íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì „ë¶€ ë°˜í™˜
    """
    if input_path.is_file():
        yield input_path
        return

    if not input_path.exists():
        return

    for pat in get_target_globs():
        for p in input_path.rglob(pat):
            if p.is_file():
                yield p


def output_path_for_input(in_file: Path, out_dir: Path) -> Path:
    """ì¶œë ¥ íŒŒì¼ëª…: pred_ + ì›ë³¸ íŒŒì¼ëª…"""
    return out_dir / f"pred_{in_file.name}"


def _contains_any(text: str, cues: List[str]) -> int:
    """ë‹¨ìˆœ í‚¤ì›Œë“œ í¬í•¨ ê°œìˆ˜(ì ìˆ˜)"""
    t = (text or "").lower()
    score = 0
    for w in cues:
        if w.lower() in t:
            score += 1
    return score


def decide_surprise_to_4(text: str, text_pair: Optional[str]) -> Tuple[str, Dict[str, int]]:
    """
    ë†€ëŒ(SURPRISE)ì„ 4ê°ì •(F/E/N) ì¤‘ ì–´ë””ë¡œ ë³´ë‚¼ì§€ ê²°ì •í•˜ëŠ” í›„ì²˜ë¦¬ ê·œì¹™.
    - ìœ„í˜‘ ë‹¨ì„œê°€ ë” ê°•í•˜ë©´ F
    - ë³´ìƒ ë‹¨ì„œê°€ ë” ê°•í•˜ë©´ E
    - ë‘˜ ë‹¤ ì•½í•˜ë©´ N
    """
    combined = (text or "")
    if text_pair:
        combined = combined + "\n" + str(text_pair)

    threat_score = _contains_any(combined, THREAT_CUES)
    reward_score = _contains_any(combined, REWARD_CUES)

    if threat_score > reward_score and threat_score > 0:
        return "F", {"threat_score": threat_score, "reward_score": reward_score}
    if reward_score > threat_score and reward_score > 0:
        return "E", {"threat_score": threat_score, "reward_score": reward_score}

    # ë‘˜ ë‹¤ ì• ë§¤í•˜ë©´ N
    return "N", {"threat_score": threat_score, "reward_score": reward_score}


def probs7_to_probs4_with_postprocess(
    probs7: List[float],
    text: str,
    text_pair: Optional[str],
) -> Dict[str, Any]:
    """
    í•µì‹¬ í›„ì²˜ë¦¬:
    1) 7ê°œ í™•ë¥ (probs7)ì„ 4ê°œ í™•ë¥ (probs4)ë¡œ í•©ì‚°
    2) ë‹¤ë§Œ SURPRISE í™•ë¥ ì€ ê·œì¹™ìœ¼ë¡œ F/E/N ì¤‘ í•˜ë‚˜ì— "ì „ë¶€" ë”í•œë‹¤.

    ë°˜í™˜:
    - probs4: [P(N), P(F), P(A), P(E)] (í•©=1)
    - surprise_to: SURPRISEê°€ ì–´ë””ë¡œ ê°”ëŠ”ì§€
    - cue_scores: ìœ„í˜‘/ë³´ìƒ ì ìˆ˜
    """
    # 4ê°ì • í™•ë¥  ëˆ„ì (ìˆœì„œ ê³ ì •)
    pN = 0.0
    pF = 0.0
    pA = 0.0
    pE = 0.0

    # ë¨¼ì € SURPRISE ì™¸ë¥¼ ê¸°ë³¸ ë§¤í•‘ìœ¼ë¡œ ëˆ„ì 
    for idx, p in enumerate(probs7):
        lab7 = ID2LABEL_7[idx]
        if lab7 == "SURPRISE":
            continue
        lab4 = MAP_7_TO_4_BASE[lab7]
        if lab4 == "N":
            pN += p
        elif lab4 == "F":
            pF += p
        elif lab4 == "A":
            pA += p
        elif lab4 == "E":
            pE += p

    # SURPRISE í™•ë¥ ì€ ê·œì¹™ìœ¼ë¡œ ë¶„ë°°(ì—¬ê¸°ì„  í•œ ê³³ì— ëª°ì•„ì¤Œ)
    p_surprise = probs7[1]  # SURPRISE index=1
    surprise_to, cue_scores = decide_surprise_to_4(text, text_pair)

    # SURPRISE_MIN_PROB ì„¤ì •ì— ë”°ë¼ "ë†€ëŒ ë¶„ê¸°"ë¥¼ ì œí•œí•  ìˆ˜ë„ ìˆìŒ
    if p_surprise < SURPRISE_MIN_PROB:
        # ë†€ëŒì´ ì•½í•˜ë©´ ê·¸ëƒ¥ Nì— ë³´ë‚´ëŠ” ì‹(ì›í•˜ë©´ ë‹¤ë¥¸ ì •ì±… ê°€ëŠ¥)
        surprise_to = "N"

    if surprise_to == "N":
        pN += p_surprise
    elif surprise_to == "F":
        pF += p_surprise
    elif surprise_to == "A":
        pA += p_surprise
    elif surprise_to == "E":
        pE += p_surprise

    # ì •ê·œí™”(í•©ì´ 1ì´ ë˜ê²Œ)
    s = pN + pF + pA + pE
    if s > 0:
        pN, pF, pA, pE = pN / s, pF / s, pA / s, pE / s

    probs4 = [pN, pF, pA, pE]
    pred4 = ["N", "F", "A", "E"][int(torch.tensor(probs4).argmax().item())]

    return {
        "pred4": pred4,
        "probs4": probs4,
        "surprise_to": surprise_to,
        "cue_scores": cue_scores,
    }


def predict_one(
    model,
    tokenizer,
    device,
    text: str,
    text_pair: Optional[str],
    max_length: int,
) -> Dict[str, Any]:
    text = (text or "").strip()
    if not text:
        return {"_skip": True}

    if text_pair is not None:
        text_pair = str(text_pair).strip()
        if not text_pair:
            text_pair = None

    if text_pair is None:
        enc = tokenizer(
            text,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )
    else:
        enc = tokenizer(
            text,
            text_pair=text_pair,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )

    enc = {k: v.to(device) for k, v in enc.items()}

    with torch.no_grad():
        logits = model(**enc).logits[0]  # (7,)
        probs7 = torch.softmax(logits, dim=-1).detach().cpu().tolist()
        pred_id = int(torch.argmax(logits).item())

    pred7 = ID2LABEL_7[pred_id]  # FEAR/SURPRISE/...

    # âœ… í›„ì²˜ë¦¬ë¡œ 7->4 ë³€í™˜(ë†€ëŒ ë¶„ê¸° í¬í•¨)
    pp = probs7_to_probs4_with_postprocess(probs7, text, text_pair)

    return {
        "pred7": pred7,
        "probs7": probs7,
        "pred4": pp["pred4"],
        "probs4": pp["probs4"],
        "surprise_to": pp["surprise_to"],
        "cue_scores": pp["cue_scores"],
    }


def run_one_file(
    model,
    tokenizer,
    device,
    in_path: Path,
    out_path: Path,
    batch_size: int,
    max_length: int,
) -> Tuple[int, int]:
    rows = load_lines(in_path)
    out_rows: List[Dict[str, Any]] = []

    bs = max(1, int(batch_size))
    for start in range(0, len(rows), bs):
        batch = rows[start:start + bs]
        for b in batch:
            result = predict_one(
                model=model,
                tokenizer=tokenizer,
                device=device,
                text=b.get("text", ""),
                text_pair=b.get("text_pair"),
                max_length=max_length,
            )
            if result.get("_skip"):
                continue
            out = dict(b)
            out.update(result)
            out_rows.append(out)

    write_jsonl(out_rows, out_path)
    return len(rows), len(out_rows)


def main() -> None:
    # âœ… ìŠ¤ìœ„ì¹˜ ì²´í¬
    if not RUN_OFFENDER_PAIR and not RUN_THOUGHTS_PAIR:
        print("[error] RUN_OFFENDER_PAIR / RUN_THOUGHTS_PAIR ë‘˜ ì¤‘ í•˜ë‚˜ëŠ” Trueì—¬ì•¼ í•©ë‹ˆë‹¤.")
        return

    input_path = Path(INPUT_PATH)
    out_dir = Path(OUTPUT_DIR)
    out_dir.mkdir(parents=True, exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"[info] device={device}")

    tokenizer = AutoTokenizer.from_pretrained(
        TOKENIZER_ID,
        use_fast=True,
        trust_remote_code=True,
    )

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_ID,
        trust_remote_code=True,
    )
    model.to(device)
    model.eval()

    files = list(iter_input_files(input_path))
    if not files:
        print(f"[warn] no input files found: {input_path}")
        print(f"       patterns: {get_target_globs()}")
        return

    total_in = 0
    total_out = 0

    print(f"[info] patterns: {get_target_globs()}")
    for in_file in files:
        out_file = output_path_for_input(in_file, out_dir)

        in_n, out_n = run_one_file(
            model=model,
            tokenizer=tokenizer,
            device=device,
            in_path=in_file,
            out_path=out_file,
            batch_size=BATCH_SIZE,
            max_length=MAX_LENGTH,
        )

        total_in += in_n
        total_out += out_n

        print(f"[done] {in_file.name}")
        print(f"       in={in_n} out={out_n}")
        print(f"       -> {out_file.name}")

    print("\n=== all done ===")
    print(f"  files: {len(files)}")
    print(f"  total_in_rows:  {total_in}")
    print(f"  total_out_rows: {total_out}")
    print(f"  output_dir: {out_dir}")


if __name__ == "__main__":
    main()
